<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: research | Danny Navarro's blog]]></title>
  <link href="http://dannynavarro.net/categories/research/atom.xml" rel="self"/>
  <link href="http://dannynavarro.net/"/>
  <updated>2014-03-11T17:25:58+01:00</updated>
  <id>http://dannynavarro.net/</id>
  <author>
    <name><![CDATA[Danny Navarro]]></name>
    <email><![CDATA[j@dannynavarro.net]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[The bioinformatics curse]]></title>
    <link href="http://dannynavarro.net/2011/01/11/the-bioinformatics-curse/"/>
    <updated>2011-01-11T00:00:00+01:00</updated>
    <id>http://dannynavarro.net/2011/01/11/the-bioinformatics-curse</id>
    <content type="html"><![CDATA[<p>As a bioinformatician you will be considered a programmer by biologists and a biologist by programmers. When talking with programmers you will suck at programming, when talking with biologists you will suck at biology. Biologists don&rsquo;t want to know much about computing, understandably, they want to get their job done. Programmers might show some curiosity in biology but tend to shield themselves from biology complexity in order to get to get work done. As a bioinformatician you have to know enough of biology to be in the cutting-edge so what you research continues being relevant and keep improving your programming skills so you are still productive for what is expected of a programmer nowadays.</p>

<p>Some influential bioinformaticians group try to define the bioinformatics field as if it were precisely the research they are doing, frowning upon bioinformatics research not similar to theirs (or similar but superior to theirs). The followers of these groups try to imitate them so that they can be some day become <em>experts</em> in the field. I see also other bioinformaticians gathering together just because they work in biology using a computer, regardless of how little overlap there is in the things they do. It&rsquo;s like group therapy, sharing experiences with people marginalized for the same reason.</p>

<p>Bioinformatics field is still in the very beginning. The field is very broad and will eventually be fragmented in multiple <em>official</em> fields. Working in an emerging field can be very exciting because you don&rsquo;t have the constrains rules of an established field. But if social recognition is important to you, think twice when getting into bioinformatics. You&rsquo;d likely feel out-of-place wherever you go.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Sharing proteomics data, trickier than it seems]]></title>
    <link href="http://dannynavarro.net/2010/07/31/sharing-proteomics-data-trickier-than-it-seems/"/>
    <updated>2010-07-31T00:00:00+02:00</updated>
    <id>http://dannynavarro.net/2010/07/31/sharing-proteomics-data-trickier-than-it-seems</id>
    <content type="html"><![CDATA[<p>Reading the <a href="http://cameronneylon.net/blog/metrics-of-use-how-to-align-researcher-incentives-with-outcomes/">blog post</a> from <a href="http://cameronneylon.net">Cameron Neylon</a> about how research incentives should align with research outcomes made me see a clear relation with the problem of sharing data in proteomics.</p>

<p>Accessing genomics or transcriptomics data is more or less straightforward compared to proteomics data. You go to a data repository and download whatever you are looking for. Anyone who tries to do something similar in proteomics usually ends up downloading spreadsheets with incomplete data from supplemental material of published articles.</p>

<p>It&rsquo;s frequent to hear proteomics informaticians complaining about experimentalists not making their data easily available and how they could be so greedy of not publishing the data for what they have been funded with public money. But jumping into conclusions about experimentalists being greedy or sloppy is not fair to me. The real issue is far deeper than that.</p>

<p>First of all, in order to share proteomics data the first requirement is to have the proper infrastructure. Lately there has been some projects aiming to provide the sharing infrastructure. I would say that the most popular proteomics repositories are <a href="http://www.peptideatlas.org/">PeptideAtlas</a>, <a href="http://www.humanproteinpedia.org/">Proteinpedia</a>, <a href="http://www.ebi.ac.uk/pride/">PRIDE</a> and <a href="https://proteomecommons.org/">ProteomeCommons</a>.</p>

<p>As far as I know, the main way Proteinpedia and PeptideAtlas handle data upload is with manual curators who study the format and the way the proteomics experiments were done. Then they come up with the best way to put the data in the repository. Usually is a combination of custom parsing with some manual editing to make experiments consistent in the repository. Obviously this kind of method is not that scalable for every proteomics experiment out there. In order to cope Proteinpedia focuses on human submissions only, whereas PeptideAtlas team is already looking into other <a href="http://www.peptideatlas.org/upload/">solutions</a> to facilitate data submission.</p>

<p>PRIDE, in the other hand, accept only data submission in PRIDE XML format. The generation of this XML format is not supported by most of the proteomics tools experimentalists use. However the PRIDE team provide an <a href="http://code.google.com/p/pride-converter/">application</a> that tries to convert every format out there and every experiment design to the PRIDE XML format. But because they try cover every single aspect of every experiment, I&rsquo;ve seen experimentalists struggle when trying to fill all the forms. Moreover, they also don&rsquo;t like the rigidity of the model to depict what the proteomics experiment was about. It&rsquo;s true that there are some optional controlled vocabulary terms to give flexibility but still experiments have difficulties wrapping their head about how to use those terms.</p>

<p>This problem is not exclusive of PRIDE. Each proteomics lab uses its own mass spec terminology and frequently forms designed by developers with no first-hand experience making experiments don&rsquo;t match the terms the experimentalists would understand. After all, experimentalists like to spend their time with experiments, they don&rsquo;t like spending their time in things that are considered bureaucracy. The PRIDE team is aware of this problem a keeps trying with more <a href="http://www.ebi.ac.uk/pride/proteomeharvest/">familiar ways</a> for experimentalists to fill form data.</p>

<p>ProteomeCommons seems to be the repository getting most traction. It&rsquo;s currently where most proteomics experimentalists are submitting their data to fend off journal editors' complains about the lack of published data. ProteomeCommons is built on top of the <a href="https://trancheproject.org/">Tranche</a> network, a kind of global distributed filesystem, that potentially offers infinite scalability to store data by just adding more nodes to the network. The tranche network looks just like a big hard disk with several files. Everybody can upload anything they like. That&rsquo;s where ProteomeCommons comes in place, it&rsquo;s the web gateway to upload the data, the web application offers some options to annotate the data, but it&rsquo;s not as complete as what you have in the other databases. That&rsquo;s understandable because if they bothered the experimentalists with thousands of forms with mandatory fields, the experimentalist wouldn&rsquo;t submit their data to ProteomeCommons. It&rsquo;s also worth mentioning that ProteomeCommons is not required to use Tranche network, any other proteomics repository could use Tranche network as a backend to store huge proteomics files. The rest of the repositories are currently looking into using the Tranche network to store the huge amounts of proteomics data, specially derived from raw data.</p>

<p>You might have notice the conspicuous absence of format standards in my description about the different repositories. If everybody used the same proteomics standards the infrastructural problems to share data would have been solved. Right?</p>

<p>The major effort to standardized proteomics formats is being carried out by <a href="http://www.psidev.info/">HUPO-PSI</a>. It&rsquo;s a kind of consortium where they have regular meetings where representatives of different proteomics groups among the world agree on what has to go in the standard and how. You can follow the discussions and chip in for what would you like to have in the formats.</p>

<p>Aside of the typical problems of something <a href="http://www.codinghorror.com/blog/2005/06/the-pontiac-aztek-and-the-perils-of-design-by-committee.html">designed by committee</a>, it remains to be seen if mass spec vendors and proteomics software developers will fully embrace the standards. Proteomics data is highly heterogeneous by nature, there are very different kinds of proteomics experiments depending on what is the research being done. High quality proteomics experiments is not something that can be converted into an assembly line process where everything can be easily is fixed and standardized.</p>

<p>However the new stable <a href="http://www.psidev.info/index.php?q=node/257">format</a> <a href="http://www.psidev.info/index.php?q=node/403">releases</a> from HUPO PSI look good enough to me to at least start making the data exchange among repositories possible. There is also the promise from several mass spec vendors of future commitment to fully support the standards. I hope all those promises don&rsquo;t end up in just that.</p>

<p>In my opinion all these infrastructural difficulties are going to be solved somehow relatively soon. The problem is that I don&rsquo;t think that just by solving the infrastructural problems everybody will start sharing data transparently. There are other difficulties.</p>

<p>The main reason experimentalists are, at least, uploading their data to Tranche is to avoid being bugged by proteomics journals into making their proteomics data available. Many proteomics journals are getting really <a href="http://www.mcponline.org/site/misc/ParisReport_Final.xhtml">serious</a> about this making the data available. Why proteomics journals are so interested in having the authors making their data available?</p>

<p>One could argue that editors of these journals believe in the moral imperative of making the data available but I don&rsquo;t buy ethics as the main reason. The majority of biomedical scientific journals are still for profit companies, not academic institutions. In order to survive they have to make money selling something as every other company. For a journal publishing articles with lots of citations from other journals, with lots citations themselves, are the best way to guarantee that companies and institutions will keep renewing the yearly subscriptions.</p>

<p>It&rsquo;s not something that I can&rsquo;t demonstrate with facts, but lately I&rsquo;m getting the feeling that proteomics is being disregarded by people in other biological fields as <em>low quality research</em>. After all proteomics is just a technology, a tool, to find out biological insights. Mass spec research by itself wouldn&rsquo;t get so much funding if it couldn&rsquo;t be used for biological research. What I think it&rsquo;s happening is that biologists are taking less seriously biological findings in pure proteomics journals. Most published proteomics experiments are irreproducible and if you start digging into the published data you frequently find many false positives. That&rsquo;s why proteomics journals editors are enforcing the experimentalists to release their data and make it as transparent as possible. They hope they can gain more credibility and get more citations from non-proteomics journals.</p>

<p>But still one can see that most experimentalists are reluctant to make their best data fully available. Many informaticians trying to analyze the data think that this attitude is because of cultural resistance to change. Many of these informaticians try to evangelize the experimentalists about why is so important to share data. Among evangelists the most notable group is the <a href="http://www.fixingproteomics.org/">Fix Proteomics Campaign</a>, which proposes some habits to make proteomics more credible.</p>

<p>But experimentalists are not dummies. I have seen them changing really quickly any habit if they find something better. The problem is that making their data transparently available is worse for them and here is where I think the campaigns miss the point. Let me explain something unique about mass spectrometry proteomics that seems easily forgotten by many people.</p>

<p>Mass spectrometers are really expensive instruments. Getting the adequate skills to operate them takes several years of training. To make things more costly these instruments become obsolete in a matter of few years because there new ones are constantly new ones coming up with better features. When a new instrument arrives to the lab a lot of time is spent optimizing it and learning how to troubleshoot it. If you don&rsquo;t keep getting those new mass spectrometers you are left behind by the competitors because they can get advantage of more powerful instruments.</p>

<p>How in academics is possible to maintain a high funding inflow? In a company you have to sell a a service or a product but you can&rsquo;t do so in an academic group. Usually you rely on grant agencies to provide funding. Granting agencies grant money by scientific productivity of the group. Publications in <em>reputable</em> journals are the main tangible measurement used by granting agencies for scientific productivity.</p>

<p>But most journals, as I said before, have to operate like companies. Proteomics data by itself is not publishable, if there is no story with some biological insight or some novel way to improve results, what will you write in a proteomics paper with just high quality data? Generating high quality proteomics data is damn difficult, I would say even more difficult than to come with fancy analysis of data. Let me explain.</p>

<p>People coming from genomics and transcriptomics fields sometimes forget that the chemical nature of proteins is much more diverse than DNA or RNA. After all, DNA and RNA have more or less homogeneous chemical properties regardless of its sequence. Proteins, in the other hand, are chemically completely different from each other depending on the sequence &ndash; that&rsquo;s why they can carry out so many molecular functions. A proteins from the nucleus are completely different than the proteins from the cytoplasmic membrane.</p>

<p>The proteome is also much more dynamic than the genome. The same cell under different conditions show completely different proteome profile. You also have to take chemical modifications of proteins into account, which are only detectable by probing the proteins directly. Chemical modifications like phosphorylation act as functional switches for proteins, a protein with a modification has also different chemical properties than the same protein without modification. The heterogeneity of proteins makes protein purification, proper separation, and identification by mass spec an entire field by itself.</p>

<p>So an experimentalist, who has had years of training just to be able to identify proteins and chemical modifications, might, understandably, lack the skills for sophisticated analysis that will make the story sexier for proteomics journals. Software to analyze proteomics data as a <em><a href="http://en.wikipedia.org/wiki/Power_user">power user</a></em><em>, </em>without programming knowledge, is still in the early days. As a developer I can see how difficult it is to make analysis software that covers every kind of proteomics experiment with a <em>point and click</em> interface.</p>

<p>The most logical step for an experimentalist when having good data would be to look for people specialized in analysis to come up with a powerful story. Usually the best people analyzing are independent proteomics informaticians that would do the analysis only if they get the credit for it. After all they have to also have to get funded to keep doing research and they can always claim they are the ones writing the paper. But even if proteomics informaticians give proper credit to the data generators in their papers &ndash; which I wouldn&rsquo;t say it&rsquo;s always true &ndash;, very few granting agencies will keep the experimental lab funded just to generate data that other people will use to write publications.</p>

<p>To tackle this problem the the most powerful experimental proteomics labs are trying to aggressively hire programmers who can do analysis <em>in-house</em> so that the credit remains within the group. The first problem these groups face is that there are almost no proteomics informaticians in the job market. They have to invest in people with programming skills who will eventually get the proteomics knowledge necessary to make useful programs for analysis or to be able to analyze the data themselves.</p>

<p>Also, because of the lack of programming knowledge of the experimentalists it can be tricky for them to envision which potential programmers will have the skills required to become a good analyzer, not to mention how to motivate the programmers with an excellent about why to join their field. There are also programmers, where I include myself, that precisely look actively for this highly experimental labs instead of pure informatics groups in order to get a closer understanding of experimental data by interacting directly with the experimentalists.</p>

<p>But this kind of setting where data generators and informaticians try to work together is propitious to get into <a href="http://en.wikipedia.org/wiki/Dilbert">Dilbert-kind</a> of situations. I would say that it&rsquo;s mainly because of the misunderstanding generated by the technological gap.</p>

<p>I feel fortunate of working in <a href="http://bioms.chem.uu.nl/">my current group</a> because I think it&rsquo;s one of the few experimental proteomics lab where people are aware of this problem and actively try to improve the communication with the informaticians.</p>

<p>The final point I want to make in this post is that if the data generators were rewarded properly for what they are good at, generating unbeatable high quality, sharing proteomics data transparently, and as soon as it&rsquo;s generated, would become mainstream. I see granting agencies rewards as the main cause for not sharing data because they usually don&rsquo;t reward the generation of data accordingly. They should also reward the labs that not only make the data available but make it as accessible as possible for data analyzers, so that proteomics research field would advance much faster than it&rsquo;s currently doing. I acknowledge granting agencies are changing slowly for the better but there is still a long way to go.</p>

<p>But understanding how granting agencies work and what are their motivations is still something quite fuzzy to me. I think I&rsquo;m still not old enough to understand the politics behind research funding.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[How to review supplemental material without revealing your identity]]></title>
    <link href="http://dannynavarro.net/2010/05/25/how-to-review-supplemental-material-without-revealing-your-identity/"/>
    <updated>2010-05-25T00:00:00+02:00</updated>
    <id>http://dannynavarro.net/2010/05/25/how-to-review-supplemental-material-without-revealing-your-identity</id>
    <content type="html"><![CDATA[<p>The <em>de facto</em> way to consider an article as scientifically valid is whether the publisher carried out a <a href="http://en.wikipedia.org/wiki/Peer_review">peer review</a> process or not. The reviewers are people with proven expertise in the field &ndash; by publishing peer reviewed articles in the area &ndash; who are capable of assessing the scientific value of an article. Because those reviewers can be direct competitors &ndash; who want to bog down the authors because they compete for funding or colleagues of the type: <em>you scratch my back, I scratch yours</em> &ndash; the editor doesn&rsquo;t unveil who are the reviewers. In my opinion the reviewers should be made public, both to the authors and to the readers of the article, but I would leave that for another post.</p>

<p>However keeping the reviewers anonymous these days is becoming more difficult because many scientific articles include supplemental material that can&rsquo;t be attached directly to the article, i. e. a huge list of proteins identified in a proteomics experiment. This poses a problem for editors and reviewers because by watching which <a href="http://computer.howstuffworks.com/internet/basics/question549.htm">IPs</a> are accessing the machine which is hosting the data the original authors can <a href="http://whatismyipaddress.com/">guess</a> who are the reviewers.</p>

<p>I have heard about this issue as a really huge problem for peer review process and from advocates of third party hosts with sophisticated technologies which anonymize reviewers. But it turns out that accessing any site in the web anonymously is not as obscure or complicated as it sounds.</p>

<p>A quick way &ndash; but maybe not so reliable &ndash; of anonymizing your web traffic is by googling for &lsquo;<a href="http://www.google.com/search?q=browse+anonymously">browse anonymously.</a>&rsquo; You can find many web <a href="http://en.wikipedia.org/wiki/HTTP_proxy">proxies</a> that claim to anonymize your identity. Usually you have to paste the web site you want to access anonymously and then you&rsquo;ll be redirected to the web page normally, albeit with a much slower load. The people who are hosting the server will see, at most, an IP address where the anonymous proxy is, that won&rsquo;t probably correspond to a place where any reviewers are located.</p>

<p>But I don&rsquo;t recommend using any proxy out there unless you really need to access something anonymously quickly and you have nothing in place. After all, who knows what they can do with your data, or if your IP leaks somewhere. I would recommend the use of the <a href="http://www.torproject.org/">Tor</a> network. Tor is an anonymity network where volunteers spread all over the world provide their machines to act as anonymizing proxies. Oversimplifying, Tor connects, encrypts and obfuscates the web traffic between you and the host with many of these proxies so that it becomes damn difficult to find out the original IP. When using a browser that goes through the Tor network the guys hosting the data will be seeing different random IPs all over the world with no relation whatsoever to each other.</p>

<p>There are different ways to setup Tor but if you want to use it without thinking too much go to the <a href="http://www.torproject.org/easy-download.html.en">download page</a> of Tor and get the Tor browser bundle. That will come with a firefox browser which is already configured for accessing the tor network. If in your institution or company, the network policy is controlled by a <a href="http://en.wikipedia.org/wiki/Bastard_Operator_From_Hell">fascist network administrator</a> who denies everything in the firewall regardless of the true danger for security is, go to <em>settings</em> and indicate you are behind a firewall.</p>

<p>By giving the link to the Tor bundle browser in the supplemental material the editor and the reviewers shouldn&rsquo;t have any problem accessing self-hosted data. Downloading the Tor bundle browser shouldn&rsquo;t be an obstacle, I haven&rsquo;t seen anyone complaining when asked to download a propietary viewer to visualize closed formats for raw data, which is quite common in proteomics.</p>

<p>However with this post I&rsquo;m not advocating to host your own data instead of sharing it. Hosting it yourself and sharing are not mutually exclusive. I think sharing your scientific data is a moral imperative when you are funded with public money. I encourage sending scientific data to as many public repositories as  possible, but I also think individual researchers have the right to host their own data if they want to.</p>

<p>I know it&rsquo;s hard to believe but many researchers who are funded with money coming from tax-payers are reluctanct to share their data, at least in the <a href="http://en.wikipedia.org/wiki/Proteomics">proteomics</a> community where I work. If their data is <em>stolen</em> and other people find more interesting things the original authors missed they lose the relevance necessary to keep getting funded, specially when the analyzers don&rsquo;t give enough credit to the generators of data which is quite frequent. But I will leave that for another post.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[The blogging itch]]></title>
    <link href="http://dannynavarro.net/2010/03/24/the-blogging-itch/"/>
    <updated>2010-03-24T00:00:00+01:00</updated>
    <id>http://dannynavarro.net/2010/03/24/the-blogging-itch</id>
    <content type="html"><![CDATA[<p>Starting a blog has been in my personal todo list for years. Finally, after encouraging myself with <a href="http://www.codinghorror.com/blog/2006/03/users-dont-care-about-you.html">some</a> <a href="http://www.codinghorror.com/blog/2007/10/how-to-achieve-ultimate-blog-success-in-one-easy-step.html">old</a> <a title="post" href="http://www.codinghorror.com/blog/2006/02/fear-of-writing.html" target="_self">posts</a> from Jeff Atwood (yeah, I like reading <a title="Jeff Atwood" href="http://www.codinghorror.com/blog/" target="_self">Jeff Atwood</a> and I frequently agree with him), I decided to just kicked it off with whatever I have in my mind: why it has been so difficult to get started? Why now? It was not only procrastination, there has been something else. Let me explain.</p>

<p>I started using routinely a feed reader around 4 years ago. Since then I&rsquo;ve been constantly replacing the blog feeds I follow with others I found more interesting. Right now all the blogs I&rsquo;m following are excellently written, and I&rsquo;m not talking only about celebrity blogs, but also the kind of highly technical blogs that make it, for example, into <a href="http://planet.python.org/">Python planet</a>. When I think about the quality of writing I would like for my blog several blogs from Python planet come to my mind. But even if I put myself a much lower quality threshold to get started, it turns out that writing something of acceptable quality, just the writing, is indeed harder than it seems. I can <a href="http://zenhabits.net/2009/04/seven-productivity-tips-for-people-that-hate-gtd/" target="_self">allow myself to suck</a> but can&rsquo;t go public with something I know is total crap. Recently I reached a writing level where I&rsquo;m comfortable enough to go public.</p>

<p>But obviously publishing a decent blog is not only about writing properly. You need something to write about. For me it&rsquo;s not worth writing superficially just for the sake of writing. In order to comment on something I need to have a deep understanding of the matter, otherwise I can&rsquo;t confidently give a public opinion. But for that to happen I need something I didn&rsquo;t get until recently: specialized knowledge.</p>

<p>When I started my Biochemistry degree back in Spain I liked computers as a hobby  (I was a relatively early Linux user) but my real passion was molecular biology, the wet lab. Soon I realized how much tedious manual work and luck influenced in successful biological experiments. In the other hand, <em>experiments</em> using the computer were quick and you could somehow understand much better what was going on.</p>

<p>Later I worked for <a href="http://pandeylab.igm.jhmi.edu/akhilesh.html">Akhilesh Pandey</a> developing <a href="http://hprd.org/">HPRD</a>, a human protein database that included (and still does) plenty of high quality manually curated from scientific literature not found in any other biological databases. My role was a kind of bridge between the programmers, the curators and the biological requirements for the project. My research career started totally different to what it&rsquo;s expected from a young researcher, instead of specializing in something first and from there try to understand later how what you study contributes to science in general, I had to first understand the global picture before trying to get into the nitty-gritty. That didn&rsquo;t mean I didn&rsquo;t want to get deep into different areas. The problem was that I wanted to get into too many things at the same time. After some time the fields where I really ended up focusing have become fewer. Now, instead of working in large teams, I work more isolated in a very specialized projects within the area of proteomics informatics.</p>

<p>Then if finally I ended up working in a very specialized topic was the global view experience a waste of time? Absolutely not. My way of thinking has been critically shaped since then. I&rsquo;m the kind of person who has to have a clear reason of everything I do. Now it&rsquo;s clear for me on what I want to focus on, without losing the context of everything I do. I know what I want to learn and for what. I still maintain more or less the same goals that I had when I got into research, the difference now is how I want to reach those goals. But I&rsquo;ll leave my goals for another post.</p>

<p>In the end the lack of specialized knowledge has been a important blocker to start a blog. I didn&rsquo;t feel with enough authority to write acceptable posts for concrete topics. Now in <a href="http://en.wikipedia.org/wiki/Python_(programming_language)">Python</a> and <a href="http://en.wikipedia.org/wiki/Proteomics">proteomics</a> I&rsquo;m more or less getting above the knowledge threshold where I can start writing something critically.</p>

<p>At the same type proteomics informatics is a blend of very different specialized topics. In proteomics I still don&rsquo;t totally understand how a <a href="http://www.chemguide.co.uk/analysis/masspec/howitworks.html">mass spectrometer</a> works but of so much parsing, merging, querying, filtering, and plotting mass spec data I got a good handle of what mass spec proteomics data is like. About programming I&rsquo;m far from writing something I would qualify as <em>good code</em>. Even if my coding capabilities have improved over the years I qualify my code worse than I used to. In reality my code is better now, it&rsquo;s just that, with time, I&rsquo;m becoming more critical about what I would call good code. However I&rsquo;m fully aware that being so critical with my own code is a <a href="http://www.codinghorror.com/blog/2009/07/nobody-hates-software-more-than-software-developers.html">symptom of programming competence</a>. I feel I&rsquo;m on the right track, if I keep pushing like I&rsquo;m doing I&rsquo;ll eventually get to write something I could qualify as good enough. For now I feel competent enough in Python to start saying something meaningful about programming publicly.</p>

<p>The bottom line is that now I have some specific knowledge and a decent ability for writing. One would think that these factors provide the ideal scenario to start a blog. I know that having a blog is very important for my profession but that hasn&rsquo;t be the last push to start now. I&rsquo;m writing this post right now because I need it. Let me rephrase it: I&rsquo;m not forcing myself to write a blot, I need to write.</p>

<p>What is really happening is that blogging is just part of a bigger transformation I&rsquo;m going through. I&rsquo;ve been lurking for quite some time different open source communities, initially just to learn more from people who are way better than me in programming. But lately I have been developing a great admiration for certain communities and  individuals. I need to give them something back. I&rsquo;m gradually participating more in the community. Now I need show who I am, how I see the world, what I want in life and what do I admire and why.</p>

<p>That&rsquo;s why I&rsquo;m blogging.</p>
]]></content>
  </entry>
  
</feed>
